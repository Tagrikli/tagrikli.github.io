<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Tugrul's Website</title>
        <link rel="stylesheet" href="/static/css/base.css" />

        <!-- KaTeX for LaTeX math rendering -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="
                renderMathInElement(document.body, {
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                    ],
                })
            "
        ></script>

        <!-- Highlight.js for code syntax highlighting -->
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/rose-pine.min.css"
        />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>

        <script>
            hljs.highlightAll();
        </script>

        <!-- Starry background effect 
        <script type="module" src="/static/js/stars.js"></script>
         -->
    </head>
    <body>
        <nav class="main-nav">
            <h3><a href="/">Home</a></h3>
            <h3><a href="/thought.html">Thoughts</a></h3>
            <h3><a href="/experience.html">Work Experience</a></h3>
            <h3><a href="/music.html">Music</a></h3>
        </nav>
        <div class="page">
            <div class="content"><h1>What Is a Thing?</h1>
<p><i>On constraints, meaning, and what learning algorithms might be missing.</i></p>

<hr />

<h2>I. Things as Constraint Systems</h2>

<p>
    A <i>thing</i> can be defined as a set of constraints, a grouping of limitations over axes of variation. Given two
    axes: size, with values $\mathcal{V}_{\text{size}} = \{\text{small, medium, large}\}$, and length, with values
    $\mathcal{V}_{\text{length}} = \{\text{short, long}\}$. A snake and an elephant can then be expressed as:
</p>
<blockquote>
    $$ S(x) := \bigl(\text{size}(x) = \text{small} \;\land\; \text{length}(x) = \text{long}\bigr) $$ $$ E(x) :=
    \bigl(\text{size}(x) = \text{large} \;\land\; \text{length}(x) = \text{short}\bigr) $$
</blockquote>
<p>
    Written as predicates rather than as sets, these definitions become reusable decision rules, a function that returns
    true or false for any candidate $x$. This matters because we use concepts not just to enumerate their members, but
    to <i>recognize</i> instances and to build other concepts on top of them.
</p>

<p>
    In machine learning terms, $\text{size}$ and $\text{length}$ are <i>feature extractors</i>, and the conjunction
    $\land$ is a <i>decision rule</i>. Most end-to-end models collapse both into a single learned function, which works,
    but loses the modularity that explicit feature extraction would provide.
</p>

<blockquote>
    Expressing a concept as a composed predicate, rather than as a flat classifier , keeps the parts modular, reusable,
    and interpretable. Each sub-constraint can appear in multiple concept definitions, like a shared vocabulary.
</blockquote>

<hr />

<h2>II. The World Is Continuous; Concepts Are Discrete</h2>

<p>
    The framework above assumes discrete feature values, but the world is not discrete. Size is a spectrum; so is
    colour, temperature, and almost every other quality we care about. The axes are really real-valued:
</p>

<blockquote>$$ \text{size} : \mathcal{T} \to \mathbb{R}^+ $$</blockquote>

<p>
    We handle this by learning, or choosing, a <i>discretization</i> function that carves a continuous axis into named
    categories:
</p>

<blockquote>
    $$ \text{cat}(r) = \begin{cases} \text{small}, & r < 10 \\ \text{medium}, & 10 \le r < 20 \\ \text{large}, & r \ge
        20 \end{cases} $$ </blockquote>

        <p>
            The categorical concept definitions from the previous section sit <i>downstream</i> of this step. First we
            measure;
            then we discretize; then we apply constraints. Treating discretization as an explicit, separate stage,
            rather than
            something absorbed into a smooth network, would make the system far more interpretable and auditable.
        </p>

        <p>
            This also connects to the problem of <i>qualia</i>. Some dimensions, like the redness of red, are primitive:
            they
            cannot be decomposed into other patterns and therefore cannot be articulated, only labeled. The inability to
            "compare red with blue" reflects the fact that they occupy orthogonal axes; there is no shared sub-structure
            to
            reason about across them.
        </p>

        <hr />

        <h2>III. A Linear-Algebraic View</h2>

        <p>
            Shifting to linear algebra: imagine each thing as a vector in a high-dimensional real space, where each axis
            corresponds to some measurable quality. A concept is then a <i>subspace</i>, or more precisely, a region of
            that
            space defined by constraints on certain dimensions.
        </p>

        <blockquote>
            <i>Feature extraction, in this view, is just projection, collapsing the full space onto the dimensions that
                matter
                for a given task.</i>
        </blockquote>

        <p>
            Composition and decomposition of concepts map naturally onto operations in this space. A conjunctive concept
            is an
            intersection of half-spaces. A hierarchical concept is a projection followed by a constraint, iterated.
        </p>

        <p>
            Some concepts lack a strong physical grounding. Justice, for example, has no dedicated sensory axis. Its
            representation would be sparse, near-zero on the dimensions that correspond to physical form, and dense on
            dimensions encoding relational or social structure. This is not a failure of the framework; it is a
            prediction about
            the geometry of abstract concepts.
        </p>

        <hr />

        <h2>IV. What Is Meaning?</h2>

        <p>
            Meaning, in its simplest form, is a function that maps things to things. "Fire means danger" is a mapping
            from a
            percept to a concept. "Apple (word) means apple (fruit)" is a mapping from a symbol to an object-class. Even
            the
            existential sense, "What is the meaning of life?", can be read as asking for a mapping: from the fact of
            existence
            to some purpose or value.
        </p>

        <p>
            This makes meaning look like feature extraction: both are functions from one representation to another. But
            there is
            a subtle difference. Feature extraction surfaces a property that is <i>latent in the data</i>, reshaping the
            space
            so something implicit becomes explicit. Meaning is more like <i>recall</i>: given one thing, retrieve the
            associated
            thing from memory.
        </p>

        <p>A learning system, then, needs to do both:</p>

        <ul>
            <li><b>Extraction:</b> learn to project raw input into a space where relevant constraints become separable.
            </li>
            <li>
                <b>Association:</b> learn a mapping between regions of that space, between the concept of a car and the
                concept
                of fast travel, for instance.
            </li>
        </ul>

        <p>
            But a car also <i>contains</i> its meaning in its structure: four wheels and a chassis are not just
            associated with
            "car," they are constitutive of it. This suggests that meaning is not always a separate function to learn,
            <i>it may be recoverable from a sufficiently rich representation of the thing itself</i>.
        </p>

        <hr />

        <h2>V. Classes, Instances, and Generation</h2>

        <p>
            There is a fundamental distinction between a <i>class</i> and an <i>instance</i>. The concept "apple" in our
            minds
            is a class, a set of constraints. When we reason about a scenario, we instantiate the class: we simulate a
            particular apple. When we search the world, we apply the constraints as a filter.
        </p>

        <p>
            What we learn, then, are the <i>constraints themselves</i>, and a single learned constraint-set serves
            double duty:
        </p>

        <ul>
            <li>
                <b>As a generator (grammar):</b> the constraints define what any valid instance must satisfy, enabling
                imagination, the construction of novel instances that conform to the concept.
            </li>
            <li>
                <b>As a classifier (recognizer):</b> the same constraints can be evaluated against a given input to test
                membership.
            </li>
        </ul>

        <p>
            This dual role maps naturally onto modern generative models. Diffusion models, for example, learn to map
            noise to
            valid instances conditioned on constraints, turning verbal or structural constraints into samples that
            satisfy them.
            The constraint set <i>is</i> the grammar; generation is just constrained instantiation.
        </p>

        <p>
            <b>Note on compositionality:</b> Constraints are not flat, they are hierarchical and relative. The
            constraint
            "has-wheels" presupposes the constraint "has-a-physical-form." Learning a concept means learning an entire
            dependency graph of sub-constraints, not a single decision boundary.
        </p>

        <hr />

        <h2>VI. What Current Neural Networks Are Missing</h2>

        <p>
            Contemporary deep learning is remarkably smooth: every operation is differentiable, every representation is
            a
            continuous vector, and learning is a single long gradient descent. This smoothness is a strength, it enables
            end-to-end optimization, but it is also a liability.
        </p>

        <p>
            Human concept learning, as Piaget's schema theory describes, is <i>not</i> smooth. It proceeds in stages,
            through
            discrete restructurings of prior knowledge. New experiences are first assimilated into existing schemas;
            when
            assimilation fails, accommodation occurs, the schema itself is revised. The result is a system with stable
            attractors (concepts that resist noise) and the capacity for genuine reorganization (learning that changes
            the
            structure of knowledge, not just its parameters).
        </p>

        <p>Current networks lack both. Specifically:</p>

        <ul>
            <li>
                <b>No discrete stages in learning.</b> A network trained on a new distribution shifts all its weights
                continuously; it does not partition knowledge into stable modules and restructure only what is
                necessary.
            </li>
            <li>
                <b>No explicit discretization.</b> The boundary between the continuous geometry of the world and the
                categorical
                structure of reasoning is never made explicit. This blurs the line between perception and concept
                formation.
            </li>
            <li>
                <b>No hierarchical constraint decomposition.</b> A classifier for "car" does not explicitly invoke a
                sub-classifier for "has-wheels." The hierarchy, if it exists, is implicit and opaque.
            </li>
        </ul>

        <p>
            These are not mere engineering complaints. They represent a structural mismatch between what neural networks
            learn
            and what concepts actually are, constraint systems that are discrete, hierarchical, composable, and used
            both for
            recognition and for generation.
        </p>

        <hr />

        <h2>Conclusion</h2>

        <p>
            A thing is a set of constraints over a space of variation. Concepts are hierarchical, composable constraint
            systems
            that serve simultaneously as grammars for generating instances and as classifiers for recognizing them.
            Meaning is a
            mapping between such systems, sometimes a learned association, sometimes recoverable from structure alone.
            And the
            smooth, continuous representations of current neural networks, however powerful, do not naturally implement
            any of
            this.
        </p>

        <p>
            The path toward more human-like reasoning in machines may require reintroducing what smoothness erased:
            explicit
            discretization, staged learning, and hierarchical constraint decomposition. Not as a retreat from
            gradient-based
            methods, but as a structure imposed on top of them, the way syntax is imposed on top of the continuous
            signal of
            speech.
        </p></div>
        </div>

        <!-- Image Lightbox -->
        <div class="image-lightbox" id="lightbox">
            <img src="" alt="Enlarged image" />
        </div>

        <script>
            // Image lightbox functionality
            document.addEventListener('DOMContentLoaded', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = lightbox.querySelector('img');
                
                // Get all images in articles
                const images = document.querySelectorAll('article img, .side-media-img, .paired-img');
                
                images.forEach(img => {
                    img.addEventListener('click', function() {
                        lightboxImg.src = this.src;
                        lightbox.classList.add('active');
                        document.body.style.overflow = 'hidden';
                    });
                });
                
                // Close lightbox on click
                lightbox.addEventListener('click', function() {
                    this.classList.remove('active');
                    document.body.style.overflow = '';
                });
                
                // Close on Escape key
                document.addEventListener('keydown', function(e) {
                    if (e.key === 'Escape' && lightbox.classList.contains('active')) {
                        lightbox.classList.remove('active');
                        document.body.style.overflow = '';
                    }
                });
            });
        </script>
    </body>
</html>