<!doctype html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Tugrul's Website</title>
        <link rel="stylesheet" href="/static/css/base.css" />

        <!-- KaTeX for LaTeX math rendering -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" />
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
        <script
            defer
            src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
            onload="
                renderMathInElement(document.body, {
                    delimiters: [
                        { left: '$$', right: '$$', display: true },
                        { left: '$', right: '$', display: false },
                    ],
                })
            "
        ></script>

        <!-- Highlight.js for code syntax highlighting -->
        <link
            rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/styles/rose-pine.min.css"
        />
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.11.1/highlight.min.js"></script>

        <script>
            hljs.highlightAll();
        </script>

        <!-- Starry background effect 
        <script type="module" src="/static/js/stars.js"></script>
         -->
    </head>
    <body>
        <nav class="main-nav">
            <h3><a href="/">Home</a></h3>
            <h3><a href="/thought.html">Thoughts</a></h3>
            <h3><a href="/experience.html">Work Experience</a></h3>
            <h3><a href="/music.html">Music</a></h3>
        </nav>
        <div class="page">
            <div class="content"><h1>Update Rules</h1>
<p><i>On architectures of learning systems, and their dynamics.</i></p>

<hr />

<h2>I. What is an update rule?</h2>

<p>
    In the most general sense, a machine learning model can be written as $\hat{y} = f(x,\theta)$, where $x$ is the
    input and $\theta$ is the learned internal structure. To accommodate learning, we try to converge on an internal
    state that minimizes a cost function: $c(y, \hat{y})$, where $y$ is our real values, whereas $\hat{y}$
    is the values that the model outputs.
</p>

<p>
    We start with different initial internal states in different algorithms. For example, in decision trees and Naive
    Bayes, the internal state is minimal and constructed
    <sub>grown</sub> incrementally while discovering structure from the data in the process of training. Whereas in
    neural networks or evolutionary algorithms, most of the structure exists and parameters are initialized rather
    probabilistically. Then, as the algorithm consumes the data, either its internal state is constructed or transformed
    based on the model.
</p>

<p>
    An update rule is the rule that changes the internal state of a machine learning model. In the case of neural
    networks, most common algorithms are initialized randomly. Then, during training, their parameters are
    <i>nudged</i> mostly with respect to error, parameters' contribution to error, and a learning rate, to be
    transformed into a desired function.
</p>

<p>
    The update rule, in its most simple form, which is almost a tautology, can be written as $\theta_{t+1} = \theta_t +
    \Delta \theta_t$, where $\Delta \theta_t$ is the change in the parameter. So here the question is, "What should the
    change be, and how should we compute it?"
</p>

<p>
    This is where gradient descent and its most efficient utilization, backpropagation, comes into the picture. I
    simplify gradient descent for myself as
    <i>"If I know the gradient of the loss, moving in the opposite direction must minimize it."</i>
</p>

<p>
    When we are dealing with nested functions $f(x,\theta) = f_L(f_{L-1}(\dots f_1(x)))$, as in neural networks, the
    backpropagation algorithm, made famous by the 1986 paper
    <i> <a href="https://www.nature.com/articles/323533a0"> Learning representations by back-propagating errors</a></i
    >, rescues us with an efficient utilization of the chain rule. This technique allows us to minimize a cost function
    smoothly, end-to-end, which seems to be an effective answer to the credit assignment problem.
</p>

<hr />

<h2>II. What about our brains?</h2>

<h3>Initialization of a human brain</h3>
<p>
    Since we are born with a brain, the structure that will be refined later exists beforehand for sure. This suggests
    it is not like an incremental build of neural tissue, at least after deployment<sub>or delivery</sub>. The process
    of synaptic pruning at early ages, and the fact that infants have significantly more synaptic connections than older
    individuals, tells me that an infant exists with a brain that has a higher representational capacity, like the
    flexibility of a fluid that is inviscid, able to represent anything, yet so fluid can't converge, cursed by
    dimensionality in a sense.
</p>

<p>
    Which requires an increase in viscosity, a dimensionality reduction, while it is molded by experiences that we
    categorize as <i>experience-expectant</i>, which is a core requirement for a healthy development.
</p>

<h3>Neocortex</h3>
<p>
    I learned about the consistently layered structure of the neocortex too late. When I was trying to find proposed
    models of intelligence, I came across a book called
    <a href="https://www.goodreads.com/book/show/27539.On_Intelligence">On Intelligence</a> by
    <a href="https://en.wikipedia.org/wiki/Jeff_Hawkins">Jeff Hawkins</a>. This is one of the books that inspired me
    immensely. At the time, I didn't know about his company, Numenta, but when I discovered their proposition of
    "Hierarchical Temporal Memory", my enthusiasm grew even stronger.
</p>

<p>
    Thanks to my university for allowing me to take a neurobiology course, and to the internet for exposing me to
    resources. Even though I am nowhere near comprehensive knowledge, I became familiar with neuron types, structures,
    and functions. But at the network level, I wasn't that much educated.
</p>

<p>
    In a six-layered neocortex, we see feedforward, lateral, and feedback connections. Information is processed
    hierarchically using the same structure, besides minor differences between layers. I am mostly convinced that
    functional specialization is about where the input comes from.
</p>

<h3>Updates and the Problem</h3>
<p>
    Neurons in the neocortex do not have a known brainwide signal to adjust their internal states with respect to a
    global loss function. They operate locally, with different types of plasticity like spike-timing-dependent
    plasticity, homeostatic plasticity, neuromodulator-gated plasticity, synaptic scaling, and many more.
</p>

<p>
    And the reason why we haven't achieved a local update rule, just like neurons in our brain do, is because we haven't
    been able to solve the credit assignment problem in that scenario, thus making it unscalable. And our neurons
    operating on purely on/off signals, and representing information in frequency coding, population coding, temporal
    coding, and more, makes me think that we are <b>much much way off</b> from how nature did it.
</p>

<hr />

<h2>III. Feedforward, Feedback, Lateral</h2>

<h3>Feedforward</h3>

<p>
    The most basic one, feedforward, is familiar to us. Information gets processed and delivered to higher areas. As I
    explored in
    <a href="/content/thought/what-is-a-thing.html">What Is a Thing?</a>, expressing things as predicates allows us to
    use them as decision rules. I like to think that higher layers get those predicates as inputs.
</p>

<h3>Feedback</h3>

<p>
    Even though raw sensory input is processed hierarchically, what I experience and what I can attend to is not only
    the highest abstraction, but the sensory data as well. I project the concept of an apple onto the visual of an
    apple, and I can attend to any of them. My experience feels like a projection of every layer to a unified
    representational space.
</p>

<p>
    Another thing is that, as <a href="https://en.wikipedia.org/wiki/David_Bohm">David Bohm</a> suggests in
    <a href="https://www.goodreads.com/book/show/204527.Thought_as_a_System">Thought as a System</a>, thought lacks
    proprioception. Its projections are not realized by itself and are mistaken for input. This mixing of representation
    and reality is one of the most harmful things in life, and it deserves its own writing.
</p>

<h3>Lateral</h3>

<p>
    About lateral connections: columnar excitation and inhibition patterns, sometimes described with Ricker wavelets.
    Neurons inside minicolumns excite each other and fire together, while inhibiting others. This creates competition,
    suppresses weaker representations, increases contrast, and helps the network reach a decision state. Which reminds
    me of discretization, classification in its essence, thingification.
</p>

<hr />

<h2>IV. Balance</h2>

<p>
    I must assume a balance in these functionalities. In this framework,
    <ul>
        <li><b>High feedforward</b> might result in observation without context or recognition.</li>
        <li><b>High feedback</b> might result in overwhelming bias on perception, daydreaming, or even hallucination. </li>
        <li><b>High lateral</b> might result in oversimplification, and more black-and-white thinking.</li>
    </ul>
</p>

<p>
    I also feel like both top-down and bottom-up attention mechanisms also belong here. <br>
    Bias through feedback is top-down: focusing, searching, querying. <br>
    Salience-based attention is bottom-up: driven by midbrain signals. <br>
    Both feel like dot-product-like matching mechanisms.
</p>

<hr />

<h2>So?</h2>

<p>So a brain is a complicated network. I need to do more reading and find evidence to support my thoughts.</p>
</div>
        </div>

        <!-- Image Lightbox -->
        <div class="image-lightbox" id="lightbox">
            <img src="" alt="Enlarged image" />
        </div>

        <script>
            // Image lightbox functionality
            document.addEventListener('DOMContentLoaded', function() {
                const lightbox = document.getElementById('lightbox');
                const lightboxImg = lightbox.querySelector('img');
                
                // Get all images in articles
                const images = document.querySelectorAll('article img, .side-media-img, .paired-img');
                
                images.forEach(img => {
                    img.addEventListener('click', function() {
                        lightboxImg.src = this.src;
                        lightbox.classList.add('active');
                        document.body.style.overflow = 'hidden';
                    });
                });
                
                // Close lightbox on click
                lightbox.addEventListener('click', function() {
                    this.classList.remove('active');
                    document.body.style.overflow = '';
                });
                
                // Close on Escape key
                document.addEventListener('keydown', function(e) {
                    if (e.key === 'Escape' && lightbox.classList.contains('active')) {
                        lightbox.classList.remove('active');
                        document.body.style.overflow = '';
                    }
                });
            });
        </script>
    </body>
</html>